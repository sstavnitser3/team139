{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde86a87-995d-4bc3-8706-ccbc774d6fef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b52a3230-f0c6-47a1-9eb6-fdab4e9e2371",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
   "source": [
    "\n",
    "# # Data Loading and Preprocessing for Housing Market Analysis\n",
    "\n",
    "\n",
    "# Steps to load in Dataset:\n",
    "# 1. Go to files tab\n",
    "# 2. Press upload and load in the housing market data file\n",
    "\n",
    "# drive.mount('/content/drive') # Uncomment if running in Google Colab and need to mount Google Drive\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "encoder = LabelEncoder()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c68b80c-4600-4c67-b0d4-e458d3fd7d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Data EDA/CLEANUP: Loading, Imputation, and Feature Selection\n",
    "\n",
    "# Load data using the same file name as the reference code\n",
    "data = pd.read_csv('city_market_tracker_EDA.tsv', sep='\\t')\n",
    "print(f\"Original data shape: {data.shape}\")\n",
    "\n",
    "imp = IterativeImputer(max_iter = 10, random_state = 0)\n",
    "\n",
    "# Fill missing values based on percentage of NaNs\n",
    "for col in data.columns:\n",
    "    nan_percent = data[col].isna().mean() * 100\n",
    "\n",
    "    if 0 < nan_percent <= 40:\n",
    "        # Fill with mean for columns with 0-40% missing data\n",
    "        mean_value = data[col].mean()\n",
    "        data[col].fillna(mean_value, inplace = True)\n",
    "\n",
    "    elif 41 <= nan_percent <= 70:\n",
    "        # Fill using IterativeImputer for columns with 41-70% missing data\n",
    "        reshaped = data[[col]]\n",
    "        data[col] = imp.fit_transform(reshaped)\n",
    "\n",
    "# Select numeric columns and drop columns with only one unique value\n",
    "numeric_df = data.select_dtypes(include = ['number'])\n",
    "numeric_df = numeric_df.dropna(axis = 1, how='all')\n",
    "numeric_df = numeric_df.loc[:, numeric_df.nunique() > 1]\n",
    "\n",
    "print(f\"Numeric data shape after cleaning: {numeric_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67ed15b-7064-47bf-83e7-14e8b7777ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove highly correlated features (absolute correlation > 0.75)\n",
    "corr_matrix = numeric_df.corr().abs()\n",
    "\n",
    "# Select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "# Find features to drop\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.75)]\n",
    "\n",
    "# Drop the columns from the numeric dataframe\n",
    "reduced_df = numeric_df.drop(columns=to_drop)\n",
    "\n",
    "print(f\"Columns dropped due to high correlation: {to_drop}\")\n",
    "print(f\"Reduced numeric data shape: {reduced_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7f7129-f63e-47eb-b082-f18190d2f30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Outliers (capping at 1.5*IQR)\n",
    "for col in reduced_df.columns:\n",
    "    Q1, Q3 = reduced_df[col].quantile([0.25, 0.75])\n",
    "    IQR = Q3 - Q1\n",
    "    lower, upper = Q1 - 1.5*IQR, Q3 + 1.5*IQR\n",
    "    reduced_df[col] = reduced_df[col].clip(lower, upper)\n",
    "\n",
    "# Recombine numeric and non-numeric dataframes\n",
    "non_numeric_df = data.select_dtypes(exclude=['number'])\n",
    "final_df = pd.concat([non_numeric_df, reduced_df], axis=1)\n",
    "\n",
    "print(f\"Final data shape after outlier removal and recombination: {final_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c610be-4c1f-4b63-a60b-5bbf99f0fcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Exploratory Data Analysis (EDA)\n",
    "\n",
    "\n",
    "# PLOT TOP 5 CITIES IN CALIFORNIA AND THEIR MEDIAN SALE PRICES\n",
    "\n",
    "# Filter data for California\n",
    "graph = final_df[final_df['STATE'] == 'California']\n",
    "top = graph['CITY'].value_counts().head(5).index.to_list()\n",
    "\n",
    "# Filter for top 5 cities\n",
    "top_df = graph[graph['CITY'].isin(top)].copy()\n",
    "\n",
    "# Convert date column for plotting purposes\n",
    "top_df['DATETIME'] = pd.to_datetime(top_df['PERIOD_BEGIN'])\n",
    "top_df.loc[:, 'DATETIME_DELTA'] = (top_df['DATETIME'] - top_df['DATETIME'].min()) / np.timedelta64(1,'D') / 365.25 + 2012\n",
    "\n",
    "# Filter to columns needed for graphing\n",
    "graph_df = top_df[['DATETIME_DELTA', 'MEDIAN_SALE_PRICE', 'CITY']]\n",
    "groups = graph_df.groupby('CITY')\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.margins(0.05)\n",
    "for name, group, in groups:\n",
    "    ax.plot(group.DATETIME_DELTA, group.MEDIAN_SALE_PRICE, marker='o',\n",
    "             linestyle = '', ms=2, alpha=0.7, label=name)\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_ylabel('Median Sale Price')\n",
    "ax.set_title('Median Sale Price Over Time for Top 5 CA Cities')\n",
    "ax.legend(title='City', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635c1e6d-87b2-45be-bb8b-e8c7727fa013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Linear Regression Model\n",
    "\n",
    "\n",
    "df_lr = final_df[final_df['STATE'] == 'California'].copy()\n",
    "\n",
    "# Drop unnecessary columns\n",
    "columns_to_drop = [\n",
    "    'PERIOD_BEGIN', 'PERIOD_END',\n",
    "    'STATE', 'STATE_CODE',\n",
    "    'REGION_TYPE',\n",
    "]\n",
    "\n",
    "df_lr = df_lr.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X = df_lr.drop(columns=['MEDIAN_SALE_PRICE'])\n",
    "y = df_lr['MEDIAN_SALE_PRICE']\n",
    "\n",
    "# Handle categorical variables (Label Encoding for high-cardinality, One-Hot for others)\n",
    "categorical_columns = X.select_dtypes(include=['object']).columns\n",
    "print(\"Unique categorical value counts:\")\n",
    "for col in categorical_columns:\n",
    "    print(f\"  {col}: {X[col].nunique()} unique values\")\n",
    "\n",
    "high_cardinality_cols = []\n",
    "for col in categorical_columns:\n",
    "    if X[col].nunique() > 50:  # Threshold for high cardinality\n",
    "        high_cardinality_cols.append(col)\n",
    "        le = LabelEncoder()\n",
    "        # Apply Label Encoding\n",
    "        X[col] = le.fit_transform(X[col].astype(str))\n",
    "\n",
    "# One-hot encode the remaining low-cardinality columns\n",
    "remaining_cats = [col for col in categorical_columns if col not in high_cardinality_cols]\n",
    "if len(remaining_cats) > 0:\n",
    "    X = pd.get_dummies(X, columns=remaining_cats, drop_first=True)\n",
    "\n",
    "print(f\"\\nFinal feature count: {X.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4430011b-673d-465e-aec7-eb028bc57d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {X_train.shape[0]}, Test samples: {X_test.shape[0]}\")\n",
    "\n",
    "# Train Linear Regression model\n",
    "print(\"\\nTraining Linear Regression model...\")\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = lr_model.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "lr_r2 = r2_score(y_test, y_pred)\n",
    "lr_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "lr_mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"LINEAR REGRESSION PERFORMANCE\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"R² Score: {lr_r2:.4f}\")\n",
    "print(f\"RMSE: ${lr_rmse:,.2f}\")\n",
    "print(f\"MAE: ${lr_mae:,.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
